<!DOCTYPE html>
<html lang="ja">

<head>
  <meta charset="UTF-8">
  <title>OpenCV Sample</title>
</head>

<body>
  <div>
    <label for="modelInput">先にONNX形式のモデルを読み込む。確実に読み込みが完了するのを待ってからStartボタンを押してください。</label>
    <p><input type="file" id="modelInput" accept=".onnx"></p>
  </div>
  <div>
    <button id="startAndStop">Start</button>
  </div>
  <div>
    <video id="videoInput" width="320" height="240"></video>
    <canvas id="canvasOutput" width="320" height="240"></canvas>
  </div>
  <p id="result"></p>
  <div>
    <span>参考リンク</span><br>
    <a target="_blank" rel="noopener noreferrer" href="https://deeplearning.cms.waikato.ac.nz/user-guide/class-maps/IMAGENET/">[表形式リスト]</a>
    <a target="_blank" rel="noopener noreferrer" href="https://github.com/Waikato/wekaDeeplearning4j/blob/51571a03b974f7f3cefc3be45ca91b98922d5d1c/src/main/resources/class-maps/IMAGENET.txt">[テキストリスト]</a>
  </div>
  <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.min.js"></script>
  <!-- <script async src="https://docs.opencv.org/master/opencv.js"></script> -->
  <!-- https://github.com/opencv/opencv/releases/download/4.9.0/opencv-4.9.0-docs.zip -->
  <script src="opencv-4.9.0.js"></script>
  <script src="IMAGENET.js"></script>
  <script>
    const modelInput = document.getElementById('modelInput');
    const result = document.getElementById('result');
    let session;

    // 推論結果から、値が最大のカテゴリーのインデックスを取得する
    // これはChatGPTに教えてもらった すごい！けど慣れないから読みにくい
    async function getArgMax(tensor) {
      const data = new Float32Array(
        tensor.data.buffer, tensor.data.byteOffset, tensor.data.length
      );
      const maxIndex = data.reduce((maxIndex, currentValue, currentIndex, array) => {
        return currentValue > array[maxIndex] ? currentIndex : maxIndex;
      }, 0);
      return maxIndex;
    }

    // モデルファイルを読み込む
    modelInput.addEventListener('change', async (e) => {
      const file = e.target.files[0];
      session = await ort.InferenceSession.create(await file.arrayBuffer());
    });
    
    cv['onRuntimeInitialized'] = () => {
      // ref. https://docs.opencv.org/3.4/dd/d00/tutorial_js_video_display.html
      let streaming = false;
      let video = document.getElementById("videoInput"); // video is the id of video tag
      let startAndStop = document.getElementById('startAndStop');
      let canvasOutput = document.getElementById('canvasOutput');
      let canvasContext = canvasOutput.getContext('2d');
      let src = new cv.Mat(video.height, video.width, cv.CV_8UC4);
      let dst = new cv.Mat(video.height, video.width, cv.CV_8UC4);
      let cap = new cv.VideoCapture(video);
      startAndStop.addEventListener('click', () => {
        if (!streaming) {
          onVideoStarted();
        } else {
          onVideoStopped();
        }
      });

      let timeoutID;
      function onVideoStarted() {
        const constraints = {
          audio: false,
          video: {
            // facingMode: { exact: "environment" },
            width: 640,
            height: 480
          }
        };
        navigator.mediaDevices.getUserMedia(constraints)
          .then(async function (stream) {
            video.srcObject = stream;
            await video.play();
          })
          .then(function () {
            streaming = true;
            startAndStop.innerText = 'Stop';
            // schedule the first one.
            timeoutID = setTimeout(processVideo, 0);
          })
          .catch(function (err) {
            console.log("An error occurred! " + err);
          });
      }

      function onVideoStopped() {
        const stream = video.srcObject;
        if (stream) {
          const tracks = stream.getTracks();
          tracks.forEach(function (track) {
            // stopping every track
            track.stop();
          });
        }
        video.srcObject = null;
        streaming = false;
        clearTimeout(timeoutID);
        canvasContext.clearRect(0, 0, canvasOutput.width, canvasOutput.height);
        startAndStop.innerText = 'Start';
      }

      const FPS = 30;
      async function processVideo() {
        try {
          if (!streaming) {
            // clean and stop.
            // src.delete();
            // dst.delete();
            return;
          }
          let begin = Date.now();
          // start processing.
          cap.read(src);
          // src.copyTo(dst);
          cv.cvtColor(src, dst, cv.COLOR_RGBA2RGB);
          const blob = cv.blobFromImage(
            dst, 1/255, new cv.Size(224, 224), new cv.Scalar(0, 0, 0), false, false
          );
          // ONNX Runtime Web用にデータを変換
          const imgTensor = new ort.Tensor('float32', blob.data32F, [1, 3, 224, 224]);
          // ONNXモデルに渡して推論実行
          // ここで、Python側で指定した入出力のnameを指定
          const inputs = {'modelInput': imgTensor};
          const output = await session.run(inputs);
          idx = await getArgMax(output['modelOutput']);
          // 結果を出力(カテゴリーのインデックスのみ)
          // result.innerText = JSON.stringify(output.modelOutput, null ,2);
          result.innerText = "MaxIndex: " + idx + ", [" + ImagenetClassList[idx] + "], " + output['modelOutput']['data'][idx];
      
          cv.imshow('canvasOutput', dst);
          // schedule the next one.
          let delay = 1000 / FPS - (Date.now() - begin);
          setTimeout(processVideo, delay);
        } catch (err) {
          console.log(err);
        }
      };
    };
  </script>
</body>

</html>